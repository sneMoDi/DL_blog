<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Project - Private Investigator Report</title>
    <link rel="stylesheet" href="styles.css">
    <script defer src="script.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600&display=swap" rel="stylesheet">
</head>
<body>
  <header>
    <div class="container">
      <h1>Research Project</h1>
      <button class="hamburger" onclick="toggleMenu()" aria-label="Toggle menu">‚ò∞</button>
      <nav id="navbar">
        <button onclick="scrollToSection('home')">Home</button>
        <button onclick="scrollToSection('private-investigator')">Private Investigator</button>
        <button onclick="scrollToSection('archaeologist')">Archaeologist</button>
        <button onclick="scrollToSection('diagrammer')">Diagrammer</button>
        <button onclick="scrollToSection('future-research')">Academic Researcher</button>
        <button onclick="scrollToSection('reviewers')">Reviewers</button>
        <button onclick="scrollToSection('contact')">Contact</button>
      </nav>
    </div>
  </header>
    
    <section id="home" class="hero">
        <div class="container">
            <h2>Welcome to Our Research</h2>
            <p>Decoding Language with BERT: Where Bidirectionality Meets Understanding.</p>
            <button onclick="scrollToSection('about')">Learn More</button>
        </div>
    </section>
    
    <section id="about" class="content-section">
        <div class="about-container">
            <h2>About the Project</h2>
            <p class="about-text">
                This project provides a comprehensive breakdown of the BERT paper, 
                <strong>"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"</strong> 
                authored by Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova at Google AI. 
                The project aims to analyze BERT's methodology, historical context, and real-world applications 
                through an illustrated and interactive blog. BERT revolutionized NLP by introducing a bidirectional 
                transformer-based model that understands the full context of words in a sentence, leading to significant 
                improvements in various NLP tasks. This project will delve into the details of BERT‚Äôs architecture, 
                including its masked language model (MLM) and next sentence prediction (NSP) techniques. 
                The blog will be structured into multiple sections, each playing a unique role in breaking down 
                the paper. It will include visual explanations of BERT's model structure, a historical overview of 
                transformer models leading up to BERT, an investigative look into the authors' motivations, and a 
                NeurIPS-style review assessing the strengths and limitations of the paper. Additionally, a programming 
                demonstration will showcase BERT‚Äôs real-world applications, illustrating how it can be fine-tuned for 
                tasks like sentiment analysis, question-answering, and text classification. By combining research 
                analysis with interactive visualizations, this project aims to make the complexities of BERT accessible 
                to a wider audience.
            </p>
            <div class="bert-animation">
                <p class="sentence">"BERT understands natural language."</p>
                <div class="tokenized">
                    <span class="token">[CLS]</span>
                    <span class="token">BERT</span>
                    <span class="token">understands</span>
                    <span class="token">natural</span>
                    <span class="token">language</span>
                    <span class="token">[SEP]</span>
                </div>
    
                <div class="attention-arrows">
                    <span class="arrow" id="arrow1">‚Üî</span>
                    <span class="arrow" id="arrow2">‚Üî</span>
                    <span class="arrow" id="arrow3">‚Üî</span>
                    <span class="arrow" id="arrow4">‚Üî</span>
                </div>
    
                <div class="bert-output">
                    <span class="output">[CLS]</span>
                    <span class="output">BERT*</span>
                    <span class="output">understands*</span>
                    <span class="output">natural*</span>
                    <span class="output">language*</span>
                    <span class="output">[SEP]</span>
                </div>
            </div>
        </div>
    </section>
    

    <section id="private-investigator" class="private-investigator-section">
  <div class="container">
    <h2>Private Investigator Report</h2>
    <h4> "BERT wasn‚Äôt just built. It was investigated, analyzed, and crafted by minds that knew what the future of NLP needed."</h4>
    <div class="authors">

      <a href="https://www.linkedin.com/in/jacob-devlin-135ab048/" target="_blank" class="author-link">
        <div class="author-card">
          <img src="images/Jacob.jpeg" alt="Jacob Devlin">
          <h3>Jacob Devlin</h3>
          <p>Jacob Devlin is a principal research scientist at Google AI and the lead author of the BERT paper. His work focuses on deep learning and natural language processing.</p>
        </div>
      </a>

      <a href="https://scholar.google.com/citations?user=GiCqMFkAAAAJ&hl=en" target="_blank" class="author-link">
        <div class="author-card">
          <img src="images/anime_gan_p.png" alt="Ming-Wei Chang">
          <h3>Ming-Wei Chang</h3>
          <p>Ming-Wei Chang is a senior research scientist specializing in transfer learning and multi-task learning for NLP. His work ensures that models like BERT generalize well across different tasks.</p>
        </div>
      </a>

      <a href="https://kentonl.com" target="_blank" class="author-link">
        <div class="author-card">
          <img src="images/photo.jpg.png" alt="Kenton Lee">
          <h3>Kenton Lee</h3>
          <p>Kenton Lee‚Äôs research focuses on coreference resolution and semantic parsing, contributing to BERT‚Äôs architecture and methodology.</p>
        </div>
      </a>

      <a href="http://kristinatoutanova.com" target="_blank" class="author-link">
        <div class="author-card">
          <img src="images/kristina4.jpg" alt="Kristina Toutanova">
          <h3>Kristina Toutanova</h3>
          <p>Kristina Toutanova specializes in linguistics and deep learning, bridging the gap between rule-based NLP approaches and modern neural network models.</p>
        </div>
      </a>

    </div>
  </div>
</section>

    
    <!-- üè∫ ARCHAEOLOGIST Timeline Section: Evolution of NLP -->
    <section id="archaeologist" class="timeline-section">
    <div class="container">
      <h2 class="section-title">üïµÔ∏è Archaeologist: Unearthing the Road to BERT</h2>
      <h4><p>
        BERT was the result of decades of progress from rule-based systems to statistical models to deep learning.
        It introduced the pretraining and fine-tuning paradigm that powers today‚Äôs NLP giants like RoBERTa, ALBERT, DistilBERT, and even GPT models.
      </p></h4>
      <div class="timeline">
  
        <!-- Era 1 -->
        <div class="timeline-step">
          <div class="dot"></div>
          <div class="content">
            <h3>ü™® Prehistoric Era of NLP: Rule-Based Systems</h3>
            <ul>
              <li>Used handcrafted grammar/syntax rules.</li>
              <li>Struggled to generalize across domains.</li>
              <li>Couldn‚Äôt learn as everything was hardcoded.</li>
            </ul>
          </div>
        </div>
  
        <!-- Era 2 -->
        <div class="timeline-step">
          <div class="dot"></div>
          <div class="content">
            <h3>üìä Statistical Age (1990s‚Äìearly 2010s)</h3>
            <ul>
              <li><strong>N-gram models:</strong> Used fixed window contexts (e.g., bigrams).</li>
              <li><strong>Word embeddings:</strong> Word2Vec (2013), GloVe (2014).</li>
              <li>Vectors were static, ‚Äúbank‚Äù meant the same in every context.</li>
            </ul>
          </div>
        </div>
  
        <!-- Era 3 -->
        <div class="timeline-step">
          <div class="dot"></div>
          <div class="content">
            <h3>üß† Deep Learning Boom</h3>
            <ul>
              <li><strong>LSTMs/GRUs:</strong> Captured sequence info, but slow & hard to scale.</li>
              <li><strong>Transformers (2017):</strong> ‚ÄúAttention is All You Need‚Äù changed everything: parallel, scalable, context-rich.</li>
              <li><strong>ELMo (2018):</strong> Brought contextual embeddings using bidirectional LSTMs.</li>
              <li><strong>GPT (2018):</strong> First Transformer-based model with left-to-right training.</li>
            </ul>
          </div>
        </div>
  
        <!-- Era 4 -->
        <div class="timeline-step">
          <div class="dot"></div>
          <div class="content">
            <h3>üöÄ BERT (2018)</h3>
            <ul>
              <li><strong>Masked Language Modeling (MLM):</strong> Learn from both left and right context.</li>
              <li><strong>Next Sentence Prediction (NSP):</strong> Learn relationships between sentences.</li>
              <li>Set new <strong>SOTA</strong> on 11 NLP benchmarks.</li>
              <li>Enabled plug-and-play fine-tuning across NLP tasks.</li>
            </ul>
          </div>
        </div>
    
      </div>
    </div>
    </section>
  

    <section id="future-research" class="future-research-section">
    <div class="container">
            <h2 class="section-title">Future Research Directions</h2>

            <div class="container">
              <div class="flowchart">
                <div class="node" onclick="showLinks('context')">Expanding Context</div>
                <div class="node" onclick="showLinks('multimodal')">Multimodal BERT</div>
                <div class="node" onclick="showLinks('efficient')">Efficient BERT</div>
                <div class="node" onclick="showLinks('crosslang')">Cross-Language</div>
                <div class="node" onclick="showLinks('ethics')">Ethics & Interpretability</div>
              </div>
          
              <div id="context" class="link-container">
                <a href="https://arxiv.org/abs/2004.05150" target="_blank">Longformer: The Long-Document Transformer ‚Äî Beltagy et al.</a><br>
                <a href="https://aclanthology.org/P18-1034/" target="_blank">Hierarchical Attention Networks ‚Äî Yang et al.</a>
                <p class="addition">Building on this, a potential academic project could explore hierarchical BERT models that chunk and encode long documents, then reassemble the segments using a global attention layer. This would allow BERT to tackle full-length articles or reports, a clear step forward from its sentence-level focus.</p>
              </div>
              <div id="multimodal" class="link-container">
                <a href="https://arxiv.org/abs/1908.02265" target="_blank">ViLBERT ‚Äî Lu et al.</a><br>
                <a href="https://arxiv.org/abs/1908.07490" target="_blank">LXMERT ‚Äî Tan and Bansal</a>
                <p class="addition">This opens doors to BERT-inspired models that process visual and auditory data alongside text. A promising research direction would be developing a unified encoder that captures cross-modal alignments for applications like image captioning or video Q&A.</p>

              </div>
              <div id="efficient" class="link-container">
                <a href="https://arxiv.org/abs/1905.11946" target="_blank">DistilBERT ‚Äî Sanh et al.</a><br>
                <a href="https://arxiv.org/abs/2002.08258" target="_blank">TinyBERT ‚Äî Jiao et al.</a>
                <p class="addition">Inspired by these works, a natural academic project would involve optimizing BERT through distillation, quantization, and pruning techniques for low-resource environments like smartphones or edge devices‚Äîwhere speed and memory matter most.</p>

              </div>
              <div id="crosslang" class="link-container">
                <a href="https://arxiv.org/abs/1901.07291" target="_blank">mBERT</a><br>
                <a href="https://arxiv.org/abs/2004.11867" target="_blank">XLM-R ‚Äî Conneau et al.</a>
                <p class="addition">This motivates future research into multilingual BERT models trained using meta-learning techniques to better generalize in low-resource language settings, especially for zero-shot translation and semantic understanding.</p>

              </div>
              <div id="ethics" class="link-container">
                <a href="https://arxiv.org/abs/1906.08935" target="_blank">Interpretability in Transformers</a><br>
                <a href="https://arxiv.org/abs/2103.03453" target="_blank">Bias in NLP Models</a>
                <p class="addition">Given the societal impact of large language models, an important academic extension would be to explore how interpretability tools and fairness-aware training strategies can reduce bias in BERT's predictions and make its decisions more transparent.</p>

              </div>
            </div>
    
            
            <div class="research-text">
                <p><strong>Expanding BERT's Context Window:</strong> While BERT is effective at understanding sentence-level context, future research could explore expanding its context window to handle entire documents. A modified model could incorporate hierarchical attention mechanisms to capture broader textual dependencies.</p>
    
                <p><strong>Multimodal BERT:</strong> An exciting avenue of research would be extending BERT into multimodal learning, integrating text with vision and audio. A "VisualBERT" or "Multimodal-BERT" could be trained to jointly understand text and images for applications in automated video summarization, image captioning, and human-computer interaction.</p>
    
                <p><strong>Memory-Efficient BERT Variants:</strong> While BERT delivers state-of-the-art results, its computational cost remains a challenge. Research could focus on developing efficient variants of BERT using model pruning, distillation, and quantization techniques to make deployment feasible on edge devices and mobile applications.</p>
    
                <p><strong>Cross-Language Adaptability:</strong> Current multilingual BERT models still struggle with zero-shot language understanding in low-resource languages. Future work could focus on improving BERT‚Äôs transfer learning capabilities by incorporating meta-learning techniques or unsupervised domain adaptation.</p>
    
                <p><strong>Ethical and Interpretability Studies:</strong> As BERT is increasingly used in real-world applications, understanding its biases and decision-making processes is critical. Future research could explore ways to enhance interpretability and develop fairness-aware training methods to mitigate unintended biases.</p>
    
                <p>By addressing these research challenges, future iterations of BERT could become more efficient, adaptable, and versatile, paving the way for broader AI applications in healthcare, education, and creative industries.</p>
            </div>
    
    </div>
  </section>

    <section id="reviewers" class="review-section">
        <div class="container">
          <h2 class="section-title"> Reviewer Panel</h2>
          <div class="review-score">
            <h3> Overall Score: <span class="score">8/10</span></h3>
            <p class="score-justification">    This paper was evaluated based on novelty, technical depth, clarity of presentation, and empirical results.
              We considered how the proposed model improves state-of-the-art NLP systems, its methodological soundness,
              reproducibility, and the broader impact on the research community. Groundbreaking work with long-lasting impact on NLP. Minor limitations in interpretability and efficiency, but overall a solid, well-executed contribution to the field.</p>
          </div>
      
          <div class="review-card strength">
            <h3> Strengths</h3>
            <ul>
              <li>Introduced a novel bidirectional transformer architecture for pre-training language models.</li>
              <li>Significant improvement across 11 NLP tasks including GLUE, SQuAD, and SWAG.</li>
              <li>Effectively leverages Masked Language Modeling and Next Sentence Prediction tasks.</li>
              <li>Open-sourced model and code, enhancing reproducibility and community adoption.</li>
            </ul>
          </div>

          <div class="berty-avatar">
            <img src="images/berty.png" alt="Berty the AI Reviewer analyzing paper">
            <div class="speech-bubble">‚ÄúHmm... Language Understood Deeply: The BERT BlueprintüëÄ‚Äù</div>
          </div>
      
          <div class="review-card weakness">
            <h3> Weaknesses</h3>
            <ul>
              <li>NSP task is now considered less effective for sentence-level coherence modeling.</li>
              <li>Heavy computational cost for pre-training (up to 4 days on 64 TPUs).</li>
              <li>Lack of interpretability and analysis on attention heads or token representation evolution.</li>
              <li>Limited exploration of multilingual or low-resource languages in initial release.</li>
            </ul>
          </div>
      
          
        </div>
      </section>
      

    
    <section id="contact" class="content-section">
        <div class="container">
            
        </div>
    </section>
    
    <footer>
        <div class="container">
            <h2>Contact Us</h2>
            <p>barukula.s@northeastern.edu | altaf.u@northeastern.edu </p>
            <p> CS 7150 Spring 2025 Research Project. &copy; All rights reserved.</p>
        </div>
    </footer>
</body>
</html>